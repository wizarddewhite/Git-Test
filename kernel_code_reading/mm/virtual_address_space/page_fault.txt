0. Setup IDT
================================================================================
static const __initconst struct idt_data early_pf_idts[] = {
	INTG(X86_TRAP_PF,		page_fault),
};

idtentry page_fault		do_page_fault		has_error_code=1

So for PF(page fault), do_page_fault is the handler.

1. do_page_fault(regs, error_code, address) -> __do_page_fault()
================================================================================

1.1 kmmio_fault(regs, address)
================================================================================
1.2 fault_in_kernel_space(adress)
================================================================================
1.2.1 return address >= TASK_SIZE_MAX;
================================================================================

1.3 do_kern_addr_fault(regs, error_code, address)
================================================================================

1.4 do_user_addr_fault(regs, error_code, address)
================================================================================
1.4.1 down_read_trylock/down_read(&mm->map_sem)
================================================================================
1.4.2 vma = find_vma(mm, address)
================================================================================
1.4.3 fault = handle_mm_fault(vma, address, flags)
================================================================================
1.4.3.1 arch_vma_access_permitted()
================================================================================
1.4.3.2 hugetlb_fault(vma->vm_mm, vma, address, flags)
================================================================================
1.4.3.3 __handle_mm_fault(vma, address, flags)
================================================================================
1.4.4 up_read(&mm->map_sem)
================================================================================


2. hugetlb_fault(vma->vm_mm, vma, address, flags)
================================================================================
2.1 haddr = address & huge_page_mask(h)
================================================================================
2.2 ptep = huge_pte_offset(mm, haddr), ptep points to huge page or swap
================================================================================
2.3 entry = huge_ptep_get(ptep), get pte
================================================================================
2.4 ptep = huge_pte_alloc(), or allocate pte
================================================================================
2.4.1 pgd = pgd_offset(mm, addr)
================================================================================
2.4.2 p4d = p4d_offset(mm, pgd, addr)
================================================================================
2.4.3 pud = pud_offset(mm, p4d, addr)
================================================================================
2.4.4 pte = huge_pmd_share()
================================================================================
2.4.5 pte = pmd_alloc(), allocate pmd table
================================================================================
2.4.6 return pte;
================================================================================
2.5 hugetlb_no_page(), get hugetlb and set to ptep
================================================================================
2.5.1 page = find_lock_page(mapping, idx)
================================================================================
2.5.2 ptl = huge_pte_lock(h, mm, ptep)
================================================================================
2.5.3 new_pte = make_huge_pte(vma, page, )
================================================================================
2.5.4 set_huge_pte_at(mm, haddr, ptep, new_pte)
================================================================================
2.5.5 spin_unlock(ptl);
================================================================================
2.6 hugetlb_cow(), if original PTE is not writable 
================================================================================
2.6.1 pte = huge_ptep_get(ptep)
================================================================================
2.6.2 old_page = pte_page(pte)
================================================================================
2.6.3 new_page = alloc_huge_page()
================================================================================
2.6.4 copy_user_huge_page(new_page, old_page, )
================================================================================
2.6.5 set_huge_pte_at(mm, haddr, ptep, make_huge_pte(vma, new_page, 1));
================================================================================
2.6.6 page_remove_rmap(old_page, true);
================================================================================
2.6.7 hugepage_add_new_anon_rmap(new_page, vma, haddr);
================================================================================

2. __handle_mm_fault(vma, address, flags)
================================================================================
2.1 pgd = pgd_offset(mm, address)
================================================================================
2.2 p4d = p4d_alloc(mm, pgd, address)
================================================================================

2.3 vmf.pud = pud_alloc(mm, p4d, address), pud level
================================================================================
2.4 if __transparent_hugepage_enabled(vma)
================================================================================
2.4.1 vma_is_dax(vma)
================================================================================
2.5 create_huge_pud(&vmf)
================================================================================
2.6 else if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud))
================================================================================
2.7 wp_huge_pud(&vmf, orig_pud)
================================================================================
2.8 huge_pud_set_accessed(&vmf, orig_pud)
================================================================================

2.9 pmd_alloc(mm, vmf.pud, address), pmd level
================================================================================
2.10 if (pmd_nonde(*vmf.pmd) && __transparent_hugepage_enabled(vma))
================================================================================
2.11 create_huge_pmd(&vmf)
================================================================================
2.11.1 do_huge_pmd_anonymous_page(&vmf), anonymous thp
================================================================================
2.11.1 vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD), file-backed thp
================================================================================
2.12 do_huge_pmd_numa_page(&vmf, orig_pmd)
================================================================================
2.13 wp_huge_pmd(&vmf, orig_pmd)
================================================================================
2.13.1 do_huge_pmd_wp_page(&vmf, orig_pmd)
================================================================================
2.13.2 vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD), file-backed thp
================================================================================
2.13.3 __split_huge_pmd(vma, pmd, address, false, NULL), handle file COW faults
================================================================================

2.14 handle_pte_fault(&vmf, orig_pmd), pte level
================================================================================

3. do_huge_pmd_anonymous_page()
================================================================================
3.1 anon_vma_prepare(), prepare vma->anon_vma
================================================================================
3.2 khugepaged_enter(vma, vma->vm_flags), insert mm_struct to khugepaged_scan
================================================================================
3.3 if !(vmf->flags & FAULT_FLAG_WRITE) && transparent_hugepage_use_zero_page()
================================================================================
3.4 gfp = alloc_hugepage_direct_gfpmask(vma)
================================================================================
3.5 page = alloc_hugepage_vma(gfp, vma, haddr, HPAGE_PMD_ORDER)
================================================================================
3.6 prep_transhuge_page(page)
================================================================================
3.6.1 set_compound_page_dtor(page, TRANSHUGE_PAGE_DTOR)
================================================================================
3.7 __do_huge_pmd_anonymous_page(vmf, page, gfp)
================================================================================
3.7.1 pgtable = pte_alloc_one(vma->vm_mm)
================================================================================
3.7.1.1 pte = alloc_page(gfp)
================================================================================
3.7.1.2 pgtable_pte_page_ctor(pte) -> __SetPageTable
================================================================================
3.7.2 clear_huge_page(page, vmf->address, HPAGE_PMD_NR)
================================================================================
3.7.2.1 porcess_huge_page(clear_subpage) -> clear_page
================================================================================
3.7.3 __SetPageUptodate(page)
================================================================================
3.7.4 entry = mk_huge_pmd(page, vma->vm_page_prot);
================================================================================
3.7.5 page_add_new_anon_rmap(page, vma, haddr, true)
================================================================================
3.7.5.1 __SetPageSwapBacked(page)
================================================================================
3.7.5.2 atomic_set(compound_mapcount_ptr(page), 0), thp huge page count 1
================================================================================
3.7.5.3 atomic_set(&page->_mapcount, 0), normal anon page count 1
================================================================================
3.7.6 lru_cache_add_active_or_unevictable(page)
================================================================================
3.7.6.1 __lru_cache_add(page), add to lru_add_pvec
================================================================================
3.7.7 pgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, pgtable), reserve pgtable in pmd_huge_pte
================================================================================
3.7.8 set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry)
================================================================================

3. do_huge_pmd_wp_page(vmf, orig_pmd), thp on wp
================================================================================
3.0 page = pmd_page(orig_pmd)
================================================================================
3.1 trylock_page(page), if failed to lock, it means someone is touch the page
================================================================================
3.2 reuse_swap_page()
================================================================================
3.3 huge_gfp = alloc_hugepage_direct_gfpmask(vma);
================================================================================
3.4 new_page = alloc_hugepage_vma(huge_gfp, vma, haddr, HPAGE_PMD_ORDER);
================================================================================
3.5 prep_transhuge_page(new_page)
================================================================================
3.6 split_huge_pmd(vma, pmd, address), fallback: when orig_pmd is_huge_zero_pmd()
================================================================================
3.6.1 __split_huge_pmd_locked(vma, pmd, address)
================================================================================
3.6.1.1 __split_huge_zero_page_pmd(vma, haddr, pmd), is_huge_zero_pmd(*pmd)
================================================================================
3.6.1.2 old_pmd = pmdp_invalidate(vma, haddr, pmd), invalid old pmd
================================================================================
3.6.1.3 page = pmd_page(old_pmd)
================================================================================
3.6.1.4 page_ref_add(page, HPAGE_PMD_NR - 1), interesting
================================================================================
3.6.1.5 pgtable = pgtable_trans_huge_withdraw(mm, pmd), take the reserved page
================================================================================
3.6.1.6 prepare pte in pgtable
================================================================================
3.6.1.8 pmd_populate(mm, pmd, pgtable), populate pmd
================================================================================
3.7 do_huge_pmd_wp_page_fallback(vmf, orig_pmd, page),fallback: when orign_pmd is not zero
================================================================================
3.8 clear_huge_page(new_page, address, HPAGE_PMD_NR) or
================================================================================
3.9 copy_user_huge_page(new_page, page, address, vma, HPAGE_PMD_NR)
================================================================================
3.10 entry = mk_huge_pmd(new_page, vma->vm_page_prot);
================================================================================
3.11 pmdp_huge_clear_flush_notify(vma, haddr, vmf->pmd);
================================================================================
3.12 page_add_new_anon_rmap(new_page, vma, haddr, true);
================================================================================
3.13 lru_cache_add_active_or_unevictable(new_page, vma);
================================================================================
3.14 set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
================================================================================

3. PCID
================================================================================
https://kernelnewbies.org/Linux_4.14#Longer-lived_TLB_Entries_with_PCID

4. zap_page_range(vma, start, size)
================================================================================
4.1 lru_add_drain(), why we need to do this?
================================================================================
4.1.1 pvec = lru_add_drain_cpu(get_cpu()), drain pages out of the cpu's pagevecs
================================================================================
4.1.1.1 __pagevec_lru_add(pvec), add passed pages to the LRU
================================================================================
4.1.1.1 __pagevec_lru_add_fn
================================================================================
4.1.1.1 release_pages(pvec->pages, pvec->nr), put page to pcp or page allocator
================================================================================
4.1.1.1 pagevec_reinit(pvec)
================================================================================
4.1.2 put_cpu()
================================================================================
4.2 mmu_notifier_range_init(&range, CLEAR, 0, vma, vma->vm_mm, start, end)
================================================================================
4.2.1 range->vma = vma
================================================================================
4.2.2 range->event = MMU_NOTIFY_CLEAR
================================================================================
4.2.3 range->start = start
================================================================================
4.2.4 range->end = end
================================================================================
4.3 tlb_gather_mmu(&tlb, vma->vm_mm, start)
================================================================================
4.3.1 tlb->mm = mm
================================================================================
4.3.2 __tlb_reset_range(tlb)
================================================================================
4.4 update_hiwater_rss(vma->vm_mm)
================================================================================
4.5 mmu_notifier_invalidate_range_start(&range)
================================================================================
4.6 unmap_single_vma(&tlb, vma, start, end, NULL)
================================================================================
4.6.1 unmap_page_range(tlb, vma, start, end, detail)
================================================================================
4.7 mmu_notifier_invalidate_range_end(&range)
================================================================================
4.8 tlb_finish_mmu(&tlb, start,)
================================================================================
4.8.1 tlb_flush_mmu(tlb)
================================================================================

5. pgd_alloc(), pgd is allocated during fork
================================================================================
do_fork()
    copy_process()
        copy_mm()
            dup_mm()
                mm_init()
                    mm_alloc_pgd()
                        pgd_alloc()


__do_execve_file()
    bprm_mm_init()
        mm_alloc()
            mm_init()
                mm_alloc_pgd()
                    pgd_alloc()

0. Page Table
================================================================================
may take a look into __handle_mm_fault for usage

Reference:

old
https://www.kernel.org/doc/gorman/html/understand/understand006.html
https://blog.csdn.net/myarrow/article/details/8624687

new
https://lwn.net/Articles/717293/
https://lwn.net/Articles/753267/
https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.rst

Glossary:

   Kernel         Intel

   PGD            PML5
   P4D            PML4
   PUD            PDPT
   PMD            PD
   PTE            PT


PGD: Page Global Directory
P4D: Page 4-level Directory?
PUD: Page Upper Directory
PMD: Page Middle Directory
PTE: Page Table Entries

PML5: Page-Map Level 5
PML4: Page-Map Level 4
PDPT: Page Directory Pointer Table
PD:   Page Directory
PT:   Page Table

0.1 32-bit 3-level page table
================================================================================
Described in https://lwn.net/Articles/106177/.
Before 2.6 kernel.

To access a page, use following code:

struct mm_struct *mm = current->mm;
pgd = pgd_offset(mm, address);
pmd = pmd_offset(pgd, address);
pte = *pte_offset_map(pmd, address);
page = pte_page(pte);

              31  30 29              21 20              12 11                  0
              +-----+------------------+------------------+---------------------+
              |     |Page Directory    |Page Table        |Offset               |
              +-----+------------------+------------------+---------------------+
                |                 |                     |
                |                 |                     |
                |                 |                     |
                |                 |                     |
                |                 |     pte_index(addr) |      +----------+
                |                 |                     |      |          |
                |                 |                     |      |          |
                |                 |                     |      +----------+
                |                 |     pte_offset_map()+----> |pte       |
                |                 |                            +----------+
                | pmd_index(addr) |     +----------+           |          |
                |                 |     |          |           |          |
                |                 |     |          |           |          |
                |                 |     +----------+           |          |
                |     pmd_offset()+---->|          |---------->+----------+
pgd_index(addr) |                  pmd  +----------+
                |                       |          |
                |                       |          |
                |                       |          |
                |     PDPTE             |          |
                |     +----------+      |          |
                |     |          |      |          |
                |     +----------+      |          |
pgd_offset(addr)+---> |          |----->+----------+
                 pgd  +----------+
                      |          |
                      |          |
                      |          |
        mm->pgd --->  +----------+

0.2 48-bit 4-level page table
================================================================================
Described in https://lwn.net/Articles/106177/.
After 2.6 kernel.


            47               39 38              30 29              21 20              12 11                  0
            +------------------+------------------+------------------+------------------+---------------------+
            |PML4              |Page Directory Ptr|Page Directory    |Page Table        |Offset               |
            +------------------+------------------+------------------+------------------+---------------------+
                   |                    |                      |                     |
                   |                    |                      |                     |
                   |                    |                      |                     |
                   |                    |                      |                     |
                   |                    |                      |     pte_index(addr) |      +----------+
                   |                    |                      |                     |      |          |
                   |                    |                      |                     |      |          |
                   |                    |                      |                     |      +----------+
                   |                    |                      |    pte_offset_map() +----> |pte       |
                   |                    |                      |                            +----------+
                   |                    |      pmd_index(addr) |     +----------+           |          |
                   |                    |                      |     |          |           |          |
                   |                    |                      |     |          |           |          |
                   |                    |                      | pmd +----------+  *pmd     |          |
                   |    pud_index(addr) |         pmd_offset() +---->|          |---------->+----------+
                   |                    |                            +----------+
                   |                    |                            |          |
                   |                    |                            |          |
                   |                    |                            |          |
   pgd_index(addr) |                    |     +----------+           |          |
                   |                    |     |          |           |          |
                   |                    | pud +----------+   *pud    |          |
                   |       pud_offset() +---->|          |---------->+----------+
                   |                          +----------+
                   |     +----------+         |          |
                   |     |          |         |          |
                   | pgd +----------+  *pgd   |          |
   pgd_offset(addr)+---->|          |-------->+----------+
                         +----------+
                         |          |
                         |          |
                         |          |
           mm->pgd --->  +----------+

